{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Deformable DETR on CommonForms Test Split\n",
    "\n",
    "This notebook evaluates your trained Deformable DETR model using COCO-style mAP metrics:\n",
    "- mAP @ IoU 0.5:0.95 (primary metric)\n",
    "- mAP @ IoU 0.5\n",
    "- mAP @ IoU 0.75\n",
    "- Per-class AP scores\n",
    "\n",
    "**What you need:**\n",
    "1. Your Hugging Face model ID (e.g., `your-username/deformable-detr-commonforms`)\n",
    "2. Dataset name (e.g., `jbarrow/CommonForms`)\n",
    "3. Test split name (e.g., `test` or `validation`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q transformers datasets pillow torch torchvision pycocotools accelerate huggingface_hub"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Authentication (Required for Private Models)"
  },
  {
   "cell_type": "code",
   "source": "# Authenticate with Hugging Face for private models\n# Option 1: Using Colab Secrets (Recommended)\n# - Add HF_TOKEN to Colab Secrets (key icon in left sidebar)\n# Option 2: Direct login (uncomment the notebook_login line below)\n\ntry:\n    from google.colab import userdata\n    import os\n\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    os.environ['HF_TOKEN'] = HF_TOKEN\n\n    from huggingface_hub import login\n    login(token=HF_TOKEN)\n    print(\"✓ Successfully authenticated with Hugging Face using Colab Secrets\")\nexcept:\n    print(\"⚠️  HF_TOKEN not found in Colab Secrets\")\n    print(\"For private models, either:\")\n    print(\"  1. Add HF_TOKEN to Colab Secrets, or\")\n    print(\"  2. Uncomment the line below to login manually\")\n    \n    # Uncomment this line to login manually:\n    # from huggingface_hub import notebook_login\n    # notebook_login()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Change these to match your setup\n",
    "\n",
    "MODEL_ID = \"your-username/deformable-detr-commonforms\"  # Your HuggingFace model\n",
    "DATASET_NAME = \"jbarrow/CommonForms\"  # Dataset name\n",
    "TEST_SPLIT = \"test\"  # or \"validation\" if no test split exists\n",
    "\n",
    "# Optional: limit number of samples for quick testing\n",
    "MAX_SAMPLES = None  # Set to e.g., 100 for quick test, None for full evaluation\n",
    "\n",
    "# Batch size for evaluation\n",
    "BATCH_SIZE = 8  # Adjust based on your GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\nprint(f\"Loading dataset {DATASET_NAME}, split: {TEST_SPLIT}...\")\n\n# Only download the test split (not train/val)\n# Use streaming=False but only load the specific split\ntry:\n    dataset = load_dataset(\n        DATASET_NAME, \n        split=TEST_SPLIT,\n        # Optional: use streaming to avoid downloading everything\n        # streaming=True if you want to avoid full download\n    )\nexcept Exception as e:\n    print(f\"Error loading split '{TEST_SPLIT}': {e}\")\n    print(\"\\nAvailable splits:\")\n    info = load_dataset(DATASET_NAME, split=None)\n    print(info.keys())\n    raise\n\nif MAX_SAMPLES is not None:\n    dataset = dataset.select(range(min(MAX_SAMPLES, len(dataset))))\n    print(f\"Using first {len(dataset)} samples for evaluation\")\nelse:\n    print(f\"Using full {TEST_SPLIT} split: {len(dataset)} samples\")\n\nprint(f\"\\nDataset features: {dataset.features}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(f\"Loading dataset {DATASET_NAME}, split: {TEST_SPLIT}...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=TEST_SPLIT)\n",
    "\n",
    "if MAX_SAMPLES is not None:\n",
    "    dataset = dataset.select(range(min(MAX_SAMPLES, len(dataset))))\n",
    "    print(f\"Using first {len(dataset)} samples for evaluation\")\n",
    "else:\n",
    "    print(f\"Using full dataset: {len(dataset)} samples\")\n",
    "\n",
    "print(f\"\\nDataset features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tqdm.auto import tqdm\n\nprint(\"Running inference on test set...\")\npredictions = {}\n\n# Process images one at a time to avoid batching issues with variable sizes\nwith torch.no_grad():\n    for idx in tqdm(range(len(dataset)), desc=\"Inference\"):\n        sample = dataset[idx]\n        image = sample[\"image\"]\n        image_id = sample.get(\"id\", idx)\n        \n        # Preprocess single image\n        inputs = processor(images=image, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        # Run model\n        outputs = model(**inputs)\n        \n        # Post-process predictions\n        # Get actual image size\n        if isinstance(image, Image.Image):\n            target_size = torch.tensor([image.size[::-1]]).to(device)\n        else:\n            target_size = torch.tensor([image.shape[:2]]).to(device)\n        \n        results = processor.post_process_object_detection(\n            outputs,\n            threshold=0.0,  # Keep all predictions for mAP calculation\n            target_sizes=target_size,\n        )\n        \n        # Store predictions (results is a list with one element)\n        predictions[image_id] = results[0]\n\nprint(f\"✓ Generated predictions for {len(predictions)} images\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Inference on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for batching.\"\"\"\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    image_ids = [item.get(\"id\", idx) for idx, item in enumerate(batch)]\n",
    "    return images, image_ids\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(\"Running inference on test set...\")\n",
    "predictions = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, image_ids in tqdm(dataloader, desc=\"Inference\"):\n",
    "        # Preprocess images\n",
    "        inputs = processor(images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Run model\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Post-process predictions\n",
    "        target_sizes = torch.tensor([img.size[::-1] for img in images]).to(device)\n",
    "        results = processor.post_process_object_detection(\n",
    "            outputs,\n",
    "            threshold=0.0,  # Keep all predictions for mAP calculation\n",
    "            target_sizes=target_sizes,\n",
    "        )\n",
    "        \n",
    "        # Store predictions\n",
    "        for image_id, result in zip(image_ids, results):\n",
    "            predictions[image_id] = result\n",
    "\n",
    "print(f\"✓ Generated predictions for {len(predictions)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare COCO Format and Calculate mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "\n",
    "# Prepare ground truth in COCO format\n",
    "print(\"Preparing ground truth annotations...\")\n",
    "coco_images, coco_annotations = prepare_ground_truth(dataset)\n",
    "\n",
    "# Get category names if available\n",
    "if hasattr(dataset.features[\"objects\"].feature[\"category\"], \"names\"):\n",
    "    category_names = dataset.features[\"objects\"].feature[\"category\"].names\n",
    "    categories = [\n",
    "        {\"id\": i, \"name\": name} for i, name in enumerate(category_names)\n",
    "    ]\n",
    "else:\n",
    "    # Use generic category names\n",
    "    num_categories = max([ann[\"category_id\"] for ann in coco_annotations]) + 1\n",
    "    categories = [\n",
    "        {\"id\": i, \"name\": f\"class_{i}\"} for i in range(num_categories)\n",
    "    ]\n",
    "\n",
    "# Create COCO ground truth object\n",
    "coco_gt = COCO()\n",
    "coco_gt.dataset = {\n",
    "    \"images\": coco_images,\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": categories,\n",
    "}\n",
    "coco_gt.createIndex()\n",
    "\n",
    "print(f\"✓ Ground truth: {len(coco_images)} images, {len(coco_annotations)} annotations\")\n",
    "\n",
    "# Prepare predictions in COCO format\n",
    "print(\"Preparing predictions...\")\n",
    "coco_results = prepare_for_coco_detection(predictions)\n",
    "print(f\"✓ Predictions: {len(coco_results)} detections\")\n",
    "\n",
    "# Run COCO evaluation\n",
    "if len(coco_results) == 0:\n",
    "    print(\"⚠️  No predictions generated! Model may not be detecting any objects.\")\n",
    "else:\n",
    "    print(\"\\nCalculating COCO metrics...\")\n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Class Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(coco_results) > 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Per-Class Average Precision (AP @ IoU 0.5:0.95)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate per-class AP\n",
    "    per_class_ap = {}\n",
    "    \n",
    "    for cat_id, cat_info in enumerate(categories):\n",
    "        cat_name = cat_info[\"name\"]\n",
    "        \n",
    "        # Run evaluation for this category only\n",
    "        coco_eval_cat = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "        coco_eval_cat.params.catIds = [cat_id]\n",
    "        coco_eval_cat.evaluate()\n",
    "        coco_eval_cat.accumulate()\n",
    "        \n",
    "        # Get AP @ IoU 0.5:0.95\n",
    "        ap = coco_eval_cat.stats[0]  # AP @ IoU 0.5:0.95\n",
    "        per_class_ap[cat_name] = ap\n",
    "        \n",
    "        print(f\"  {cat_name:30s}: {ap:.4f}\")\n",
    "    \n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(coco_results) > 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nModel: {MODEL_ID}\")\n",
    "    print(f\"Dataset: {DATASET_NAME} ({TEST_SPLIT} split)\")\n",
    "    print(f\"Number of test samples: {len(dataset)}\")\n",
    "    print(f\"Number of ground truth annotations: {len(coco_annotations)}\")\n",
    "    print(f\"Number of predictions: {len(coco_results)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"COCO Metrics:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    metric_names = [\n",
    "        \"Average Precision (AP) @ IoU=0.50:0.95\",\n",
    "        \"Average Precision (AP) @ IoU=0.50\",\n",
    "        \"Average Precision (AP) @ IoU=0.75\",\n",
    "        \"Average Precision (AP) @ IoU=0.50:0.95 (small)\",\n",
    "        \"Average Precision (AP) @ IoU=0.50:0.95 (medium)\",\n",
    "        \"Average Precision (AP) @ IoU=0.50:0.95 (large)\",\n",
    "        \"Average Recall (AR) @ IoU=0.50:0.95 (max 1 det)\",\n",
    "        \"Average Recall (AR) @ IoU=0.50:0.95 (max 10 det)\",\n",
    "        \"Average Recall (AR) @ IoU=0.50:0.95 (max 100 det)\",\n",
    "        \"Average Recall (AR) @ IoU=0.50:0.95 (small)\",\n",
    "        \"Average Recall (AR) @ IoU=0.50:0.95 (medium)\",\n",
    "        \"Average Recall (AR) @ IoU=0.50:0.95 (large)\",\n",
    "    ]\n",
    "    \n",
    "    for name, value in zip(metric_names, coco_eval.stats):\n",
    "        print(f\"{name:50s}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\\n✓ Evaluation complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Cannot calculate metrics - no predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Sample Predictions (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def visualize_predictions(dataset, predictions, num_samples=3, score_threshold=0.5):\n",
    "    \"\"\"Visualize predictions on sample images.\"\"\"\n",
    "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    \n",
    "    for idx in indices:\n",
    "        sample = dataset[idx]\n",
    "        image_id = sample.get(\"id\", idx)\n",
    "        image = sample[\"image\"]\n",
    "        \n",
    "        # Get predictions for this image\n",
    "        pred = predictions.get(image_id, {})\n",
    "        \n",
    "        # Filter by score threshold\n",
    "        if len(pred) > 0:\n",
    "            scores = pred[\"scores\"]\n",
    "            mask = scores > score_threshold\n",
    "            boxes = pred[\"boxes\"][mask]\n",
    "            labels = pred[\"labels\"][mask]\n",
    "            scores = scores[mask]\n",
    "        else:\n",
    "            boxes = torch.tensor([])\n",
    "            labels = torch.tensor([])\n",
    "            scores = torch.tensor([])\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        # Draw predictions\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            \n",
    "            # Draw box\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), width, height,\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add label\n",
    "            label_text = f\"{categories[label.item()]['name']}: {score:.2f}\"\n",
    "            ax.text(\n",
    "                x1, y1 - 5, label_text,\n",
    "                color='white', fontsize=10,\n",
    "                bbox=dict(facecolor='red', alpha=0.7)\n",
    "            )\n",
    "        \n",
    "        ax.axis('off')\n",
    "        plt.title(f\"Image ID: {image_id} | Predictions: {len(boxes)}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualize some predictions\n",
    "if len(coco_results) > 0:\n",
    "    print(\"Visualizing sample predictions (red boxes = predictions with score > 0.5)...\\n\")\n",
    "    visualize_predictions(dataset, predictions, num_samples=3, score_threshold=0.5)\n",
    "else:\n",
    "    print(\"No predictions to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON file\n",
    "if len(coco_results) > 0:\n",
    "    results_dict = {\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"dataset\": DATASET_NAME,\n",
    "        \"split\": TEST_SPLIT,\n",
    "        \"num_samples\": len(dataset),\n",
    "        \"metrics\": {\n",
    "            \"mAP_50_95\": float(coco_eval.stats[0]),\n",
    "            \"mAP_50\": float(coco_eval.stats[1]),\n",
    "            \"mAP_75\": float(coco_eval.stats[2]),\n",
    "            \"mAP_small\": float(coco_eval.stats[3]),\n",
    "            \"mAP_medium\": float(coco_eval.stats[4]),\n",
    "            \"mAP_large\": float(coco_eval.stats[5]),\n",
    "        },\n",
    "        \"per_class_ap\": per_class_ap,\n",
    "    }\n",
    "    \n",
    "    with open(\"evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "    \n",
    "    print(\"✓ Results saved to evaluation_results.json\")\n",
    "    \n",
    "    # Download the file\n",
    "    from google.colab import files\n",
    "    files.download(\"evaluation_results.json\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}